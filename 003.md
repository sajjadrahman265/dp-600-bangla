‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á! ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶ø **DP-600 (Microsoft Fabric Analytics Engineer Associate)** ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡¶ø ‡¶Æ‡¶æ‡¶•‡¶æ‡ßü ‡¶∞‡ßá‡¶ñ‡ßá ‡¶Ü‡¶∞‡¶ì **‡¶ó‡¶≠‡ßÄ‡¶∞, ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶ì ‡¶è‡¶®‡¶æ‡¶≤‡¶ø‡¶ü‡¶ø‡¶ï‡ßç‡¶∏-‡¶´‡ßã‡¶ï‡¶æ‡¶∏‡¶°** ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¶‡ßá‡¶¨‡•§
‡¶è‡¶ó‡ßÅ‡¶≤‡ßã Fabric‚Äì‡¶è‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ, ‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡ßã ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá‚Äî‡¶§‡¶æ‡¶á ‡¶ü‡¶æ‡¶∞‡ßç‡¶Æ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ **‡¶ó‡¶≠‡ßÄ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ + ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞ + ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶á‡¶®‡¶∏‡¶æ‡¶á‡¶ü** ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡¶¨‡ßá‡•§

---

# üî• **Microsoft Fabric Terminology ‚Äî Deep Technical & Analytics Explanation (DP-600 Level)**

---

# üü© **GENERAL TERMS (DP-600 ‡¶´‡ßã‡¶ï‡¶æ‡¶∏)**

## **1. Capacity**

Fabric‚Äì‡¶è capacity ‡¶¨‡¶≤‡¶§‡ßá ‡¶¨‡ßã‡¶ù‡¶æ‡ßü **compute + storage + concurrency limit** ‡¶è‡¶∞ ‡¶Æ‡ßã‡¶ü ‡¶∏‡¶Ç‡¶Æ‡¶ø‡¶∂‡ßç‡¶∞‡¶£‡•§

### **DP-600 ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü**

* F SKU (F2, F4, F8‚Ä¶ F256) ‚Äî dedicated capacity‡•§
* ‡¶≠‡¶ø‡¶®‡ßç‡¶® workload ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá (Spark, SQL, Power BI)‡•§
* Capacity exhaustion ‡¶π‡¶≤‡ßá:

  * SQL workloads queued ‡¶π‡ßü
  * Spark jobs slow down
  * Power BI refresh throttled ‡¶π‡ßü
* Capacity metrics:

  * **vCore utilization**
  * **Memory usage**
  * **Concurrency slots**
  * **Job queue length**

‚û° ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá ‚Äî ‡¶ï‡ßã‡¶® workload capacity consume ‡¶ï‡¶∞‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßã‡¶®‡¶ü‡¶ø shared/ dedicated ‡¶®‡ßü‡•§

---

## **2. Experience**

Fabric‚Äì‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø workload ‡¶è‡¶ï‡¶ü‡¶ø ‚ÄúExperience‚Äù ‡¶Ø‡ßá‡¶Æ‡¶®:

* Data Engineering (Spark, Lakehouse)
* Data Warehouse (SQL endpoint, DW)
* Real-Time Intelligence (KQL, Eventstream)
* Power BI (Semantic model, dashboards)

‚û° DP-600‚Äì‡¶è ‚Äúmulti-experience data pipeline design‚Äù ‡¶®‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá‡•§

---

## **3. Item**

Item basically ‡¶è‡¶ï‡¶ü‡¶ø workload-‡¶è‡¶∞ resource object:

* Lakehouse
* Warehouse
* Notebook
* Dataflow Gen2
* KQL Database
* SQL endpoint
* Power BI report/model

‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø item‚Äì‡¶è‡¶∞ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶Ö‡¶™‡¶æ‡¶∞‡ßá‡¶∂‡¶® ‡¶ì capacity consumption rule ‡¶Ü‡¶õ‡ßá‡•§

---

## **4. Tenant**

Fabric instance = Microsoft Entra ID tenant

* ‡¶∏‡¶¨ workspace, capacity, users tenant-level resource‡•§
* Cross-tenant sharing restricted by governance policy.

---

## **5. Workspace**

Workspace‚Äì‡¶è admin, member, contributor, viewer‚Äî‡¶è‡¶á roles ‡¶•‡¶æ‡¶ï‡ßá‡•§
Workspaces use **capacity** and manage:

* Access control
* Item organization
* Data lineage
* Deployment pipeline (for Power BI & Semantic Models)

‚û° DP-600‚Äì‡¶è workspace permissions ‡¶™‡ßç‡¶∞‡¶ö‡ßÅ‡¶∞ ‡¶Ü‡¶∏‡ßá‡•§

---

# üü¶ **DATA ENGINEERING (DEEP TECHNICAL)**

## **6. Lakehouse**

### üîç Internal Architecture

Lakehouse =

1. **Files** (Parquet/Delta)
2. **Tables** (Delta tables)
3. **SQL endpoint**
4. **Unity catalog‚Äìlike metadata layer (OneLake catalog)**

### üîë Key Technical Points

* Uses **Delta Lake** (ACID transactions)
* Stores data in **V-Order Parquet** (Fabric optimization)
* Two tables appear:

  * *Managed table* ‚Üí data stored inside Lakehouse folder
  * *Unmanaged table* ‚Üí external or shortcut data
* Lakehouse folders:

  * *Files* (unstructured/bronze)
  * *Tables* (structured/silver/gold)

‚û° ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü Delta Lake + Lakehouse architecture‚Äì‡¶∏‡¶Ç‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá‡•§

---

## **7. Notebook**

Used for Spark execution.
Supports:

* PySpark
* SQL
* Scala
* C# .NET for Spark
* Markdown

### DP-600 Key Features

* Notebook to Pipeline conversion
* Spark monitoring
* Lakehouse integration via spark.read/write
* ML integration with Data Science experiment tracking

---

## **8. Spark Application & Spark Job**

### Spark Application

User-written code file ‚Üí runs on Spark cluster.

### Spark Job

Application ‚Üí multiple parallel jobs ‚Üí each job = tasks & stages.

**DP-600 Exam Focus:**

* Spark UI concepts
* DAG, Shuffle, Stages
* Partitioning & performance tuning
* Job definition & scheduling

---

## **9. Spark Job Definition**

A reusable Spark configuration:

* Entry file
* Arguments
* Environment variables
* Cluster settings
* Batch / streaming job support

‚û° Practically used for scheduled big data pipelines.

---

## **10. V-Order**

Fabric‚Äìspecific parquet optimization.

### Technical Benefits:

* Column clustering
* Vectorized reads
* Compression optimization
* Faster Spark + SQL query performance
* Saves storage & improves scan efficiency

‚û° ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá: Parquet vs V-Order Parquet.

---

# üüß **DATA FACTORY (ADVANCED)**

## **11. Connector**

Connects 150+ sources:

* SQL, Oracle, SAP, Salesforce
* Azure Storage, S3
* OneLake shortcuts

Key technicals:

* Supports **pushdown transformations**
* Supports staged copy via ADLS
* Supports incremental loading

---

## **12. Pipeline**

Orchestrates:

* Data movement
* Notebook execution
* Spark job
* Dataflow Gen2
* SQL stored procedures

Control flow components:

* If-condition
* For-each
* Switch
* Wait
* Until

---

## **13. Dataflow Gen2**

Power Query engine‚Äìbased ETL:

Technical capabilities:

* Direct Lake write
* On-demand compute
* Data lineage
* Incremental refresh
* Re-use Power Query M code

‚û° Often used for Gold layer transformations.

---

## **14. Trigger**

Trigger types:

* Schedule
* Event-based
* Tumble window
* Manual

---

# üü™ **DATA SCIENCE**

## **15. Data Wrangler**

Auto-generates PySpark or Pandas code for:

* Missing value handling
* Column profiling
* Data cleaning
* EDA summary

Runs inside notebook.

---

## **16. Experiment**

Logical container for ML runs.

### Includes:

* Parameters
* Metrics
* Artifacts (models)
* Code snapshot

Uses MLflow tracking system.

---

## **17. Model**

Fabric stores ML models as:

* MLflow models
* ONNX models
* Custom formats

Models can be deployed to real-time endpoints or batch inference.

---

## **18. Run**

Each execution of experiment = Run.

Tracks:

* runtime
* parameters used
* performance metrics
* artifacts produced

---

# üü• **DATA WAREHOUSE (ADVANCED)**

## **19. SQL Analytics Endpoint**

Provides:

* SQL read access over Lakehouse data
* Supports **T-SQL over Delta tables**
* Uses **TDS protocol** (Power BI connects directly)

Great for:

* BI
* Semantic model
* Reporting

---

## **20. Fabric Data Warehouse**

Enterprise-grade warehouse:

* ACID
* Indexing
* Columnstore
* Resultset caching
* Materialized views
* Time travel (Delta)

‚úî Unlimited scale-out compute
‚úî Supports T-SQL procedures, views, triggers

‚û° DP-600 heavy focus topic.

---

# üü´ **REAL-TIME INTELLIGENCE (DEEP DIVE)**

## **21. Activator**

Similar to "Logic App for Real-Time Data"
Used for:

* Data-driven alerts
* Low-code actions
* Anomaly detection triggers

---

## **22. Eventhouse**

Container for KQL databases.
Supports:

* Streaming ingestion
* Hot cache
* Cold ADLS storage
* Real-time dashboards

---

## **23. Eventstream**

Fabric‚Äôs data-in-motion pipeline:

* Ingest ‚Üí Process ‚Üí Route
* No-code transformation
* Connectors: Kafka, Event Hub, IoT Hub, Webhooks

---

## **24. KQL Database**

Optimized for:

* log analytics
* time series
* telemetry
* clickstream
* IoT data

KQL supports:

* Summarize
* Reduce
* Join
* Windowed functions

---

## **25. KQL Queryset**

Execution environment for KQL queries.
Allows sharing and saving KQL scripts.

---

# üü© **REAL-TIME HUB**

## **26. Real-Time Hub**

Centralized catalog for all streaming datasets in tenant:

* Eventstreams
* KQL databases
* Activators
* IoT streams

---

# üü¶ **ONELAKE**

## **27. Shortcut**

Pointing to external data WITHOUT copying.

Supports:

* ADLS Gen2
* S3
* Other OneLake workspaces
* Cross-region data

Benefits:

* No duplication
* Live data access
* Managed security

---

# üî• Ready for DP-600?

 
‚úÖ **Case study** ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø
 
